#!/bin/bash

#SBATCH --job-name=bloomz_lexical    # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=80              # total number of tasks across all nodes
#SBATCH --cpus-per-task=1       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --partition=gpu-a100          # Name of the partition
# #SBATCH --exclude=e01
# #SBATCH --nodelist=octopod       # Node is only available in gpu partition
#SBATCH --gpus-per-node=1                # Total number of gpus
#SBATCH --mem=50G                # Total memory allocated
#SBATCH --hint=multithread       # we get logical cores (threads) not physical (cores)
#SBATCH --time=1:00:00          # total run time limit (HH:MM:SS)
#SBATCH --array=0-79
#SBATCH --output=logs/bloomz_lexical_%a.out   # output file name
#SBATCH --error=logs/bloomz_lexical_%a.out    # error file name


echo "### Running $SLURM_JOB_NAME ###"

echo "HOSTNAME: $(hostname)"
echo
echo CUDA in ENV:
env | grep CUDA
echo

nvidia-smi

module purge
module load conda
conda --version
module load cuda/12.1
nvcc --version

# Set your conda environment
source /home/$USER/.bashrc
conda info --envs

which python
. "/home/nbafna1/miniconda3/etc/profile.d/conda.sh" && conda deactivate && conda activate llmrob
which python

set -x # print out every command that's run with a +
cd "/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/mlmm-evaluation/"
## SCRIPT TO RUN
# bash scripts/run.sh vi openai-community/gpt2 


num_langs=7
num_tasks=3
num_params=7
langs=("en" "id" "hi" "ar" "es" "fr" "de") # #langs = 7
# langs=("en" "hi" "id" "es" "fr") # #langs = 5
# langs=("en" "id" "hi" "ar" "es" "de") # #langs = 6
# langs=("de" "fr") # #langs = 1
tasks_prod_params=$((num_tasks * num_params))
lang=${langs[$SLURM_ARRAY_TASK_ID / $tasks_prod_params]}
# tasks=(xwinograd_${lang} xstory_cloze_${lang} xcopa_${lang} arc_${lang} hellaswag_${lang} mmlu_${lang} \
# truthfulqa_${lang} flores200-${lang}-en) # #tasks = 8
# tasks=(xstory_cloze_${lang} arc_${lang} hellaswag_${lang} mmlu_${lang} \
# truthfulqa_${lang} flores200-${lang}-en) # #tasks = 6
# tasks=(truthfulqa_${lang}) # #tasks = 1
# tasks=("xnli_${lang}") # #tasks = 1
tasks=(flores200-${lang}-en xstory_cloze_${lang} xnli_mcq_${lang}) # #tasks = 3
# tasks=(xnli_mcq_${lang}) # #tasks = 3
# tasks=(flores200-${lang}-en) # #tasks = 1
# tasks=(arc_${lang} hellaswag_${lang} mmlu_${lang} \
# truthfulqa_${lang} xwinograd_${lang} xcopa_${lang}) # #tasks = 6
# tasks=(xnli_${lang} xstory_cloze_${lang} xwinograd_${lang} xcopa_${lang}) # #tasks = 4

theta_content_globals=(0 0.05 0.1 0.2 0.3 0.5 0.8)

x=$((SLURM_ARRAY_TASK_ID % tasks_prod_params))
task=${tasks[$((x / num_params))]}
theta_content_global=${theta_content_globals[$((x % num_params))]}
theta_func_global=0

echo "lang: $lang"
echo "task: $task"
echo "theta_content_global: $theta_content_global"
echo "theta_func_global: $theta_func_global"



# model="hf-seq2seq"
model="hf-auto"
# model_path="bigscience/mt0-xxl-mt" 
# model_key="mt0xxlmt~0shot"
model_path="bigscience/bloomz-7b1" 
model_key="bloomz7b"


# tasks=arc_${lang},hellaswag_${lang},mmlu_${lang}
# tasks=wmt16-de-en
# tasks="xnli_${lang}"
# tasks="xwinograd_${lang},xstory_cloze_${lang},xcopa_${lang},xnli_${lang}"
# tasks="xwinograd_${lang},xstory_cloze_${lang},xcopa_${lang}"
device=cuda
# theta=0
# all_noise_params="character_level-lang=$lang,swap_theta=$theta"
limit=300

exp_key="lexical-lang=$lang,theta_content_global=$theta_content_global,theta_func_global=$theta_func_global"
dataset_dir="/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/datasets/$lang/"
dataset_file="${dataset_dir}/${task}.txt"
output_dir="/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/noiser_artifacts/lexical/$lang/$exp_key~limit-$limit/$task/"
mkdir -p ${output_dir}
all_noise_params_str="lexical-lang=$lang,theta_content_global=$theta_content_global,theta_func_global=$theta_func_global,text_file=<$dataset_file>,output_dir=<$output_dir>"



output_base_path="/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/outputs/"
mkdir -p ${output_base_path}
noised_data_outdir="$output_base_path/noised_data/$model_key/$lang/$exp_key~limit-$limit/"
mkdir -p ${noised_data_outdir}
results_outdir="$output_base_path/results/$model_key/$lang/$exp_key~limit-$limit/"
mkdir -p ${results_outdir}


python main.py \
    --model=${model} \
    --tasks=${task} \
    --model_args pretrained=${model_path} \
    --device=${device} \
    --batch_size 32 \
    --write_out \
    --model_alias ${model_path}_${lang} \
    --task_alias ${tasks} \
    --output_base_path ${noised_data_outdir} \
    --output_path ${results_outdir}/$task.json \
    --limit $limit \
    --all_noise_params_str ${all_noise_params_str} \
    
# If the task is translation, then we need to postprocess the noised_data output files to
# add in the model translation on the non-noised data
if [[ $task == *"flores200"* ]]; then
    baseline_translation_file="$output_base_path/noised_data/$model_key/$lang/baselines~limit-300/flores200-$lang-en_write_out_info.json"
    output_translation_file="$noised_data_outdir/flores200-$lang-en_write_out_info.json"
    python /export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/experiments/utils/merge_translation_log_files.py $baseline_translation_file $output_translation_file
fi



# --output_base_path is a dir where the noised data will be stored along with model outputs for each task example,
# the filename is automatically generated as task_lang_write_out.json
# --output_path is the file where the results JSON will be stored for the entire run, with the noise params as a JSON key
