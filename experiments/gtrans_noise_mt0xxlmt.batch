#!/bin/bash

#SBATCH --job-name=mt0_gtrans    # create a short name for your job
#SBATCH --nodes=2                # node count
#SBATCH --ntasks=12              # total number of tasks across all nodes
#SBATCH --cpus-per-task=1       # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --partition=gpu-a100          # Name of the partition
#SBATCH --exclude=e01
# #SBATCH --nodelist=octopod       # Node is only available in gpu partition
#SBATCH --gpus=2                # Total number of gpus
#SBATCH --mem=80G                # Total memory allocated
#SBATCH --hint=multithread       # we get logical cores (threads) not physical (cores)
#SBATCH --time=2:00:00          # total run time limit (HH:MM:SS)
#SBATCH --array=0-11
#SBATCH --output=logs/mt0_gtrans_%a.out   # output file name
#SBATCH --error=logs/mt0_gtrans_%a.out    # error file name


echo "### Running $SLURM_JOB_NAME ###"

echo "HOSTNAME: $(hostname)"
echo
echo CUDA in ENV:
env | grep CUDA
echo

set -x # print out every command that's run with a +

nvidia-smi

module purge
module load conda
conda --version
module load cuda/10.2
nvcc --version

# Set your conda environment
source /home/$USER/.bashrc
conda info --envs

which python
. "/home/nbafna1/miniconda3/etc/profile.d/conda.sh" && conda deactivate && conda activate llmrob2
which python

cd "/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/mlmm-evaluation/"
## SCRIPT TO RUN
# bash scripts/run.sh vi openai-community/gpt2 

### Source and target languages

# Spanish:  *Catalan, Galician, Portuguese
#   Hindi:   Assamese, *Bengali, Bhojpuri, Dhivehi,  Gujarati, *Marathi, Punjabi, Sindhi, Sinahala, Oriya, *Nepali, Urdu, Maithili
#   Russian:   Belorussian, Bosnian, Bulgarian, *Croatian, Czech, Polish, *Slovak, Slovenian, *Serbian, *Ukrainian, Macedonian
# Indonesian:    Tagalog, Cebuano, Malay, Malagasy, Maori, Ilocano, Javanese, Filipino, Hawaiian
#   English:    ?Dutch, Frisian, Afrikaans,      (and possibly) Danish, German, Norwegian, Swedish, Icelandic


src_tgt_pairs=("ru:be" "ru:cs" "ru:bg" "es:ca" "es:gl" "es:pt" "id:tl" "id:ceb" "en:af" "en:fy" "hi:bho" "hi:mai")
src_tgt_pair=${src_tgt_pairs[$SLURM_ARRAY_TASK_ID]}
src_lang=$(echo $src_tgt_pair | cut -d':' -f1)
tgt_lang=$(echo $src_tgt_pair | cut -d':' -f2)
lang=$src_lang

echo "Running for $src_lang -> $tgt_lang"

model="hf-seq2seq"
model_path="bigscience/mt0-xxl-mt" 
model_key="mt0xxlmt~0shot"
tasks=(flores200-${lang}-en) # #tasks = 1
task=${tasks[0]}

# tasks=arc_${lang},hellaswag_${lang},mmlu_${lang}
device=cuda

all_noise_params="gtrans-src=${src_lang},tgt=${tgt_lang}"
limit=200

output_base_path="/export/b08/nbafna1/projects/llm-robustness-to-xlingual-noise/outputs/"
mkdir -p ${output_base_path}
noised_data_outdir="$output_base_path/noised_data/$model_key/$lang/$all_noise_params~limit-$limit/"
mkdir -p ${noised_data_outdir}
results_outdir="$output_base_path/results/$model_key/$lang/$all_noise_params~limit-$limit/"
mkdir -p ${results_outdir}

export TOKENIZERS_PARALLELISM=true
python main.py \
    --model=${model} \
    --tasks=${tasks} \
    --model_args pretrained=${model_path} \
    --device=${device} \
    --batch_size 4 \
    --write_out \
    --model_alias ${model_path}_${lang} \
    --task_alias ${task} \
    --all_noise_params_str ${all_noise_params} \
    --output_base_path ${noised_data_outdir} \
    --output_path ${results_outdir}/$task.json \
    --limit $limit
    

# --output_base_path is a dir where the noised data will be stored along with model outputs for each task example,
# the filename is automatically generated as task_lang_write_out.json
# --output_path is the file where the results JSON will be stored for the entire run, with the noise params as a JSON key
